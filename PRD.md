# Contextual RAG Library - PRD

## Problem

Traditional RAG destroys context when chunking. Example: a chunk contains "The company's revenue grew by 3% over the previous quarter" but doesn't specify which company or which quarter. This makes retrieval fail because the chunk lacks the context needed to match relevant queries.

## Solution: Contextual Retrieval

Two sub-techniques working together:

### 1. Contextual Embeddings
Before embedding each chunk, prepend a short context (100-200 tokens) that situates the chunk within its parent segment. 

Example transformation:
- **Original chunk**: "The company's revenue grew by 3% over the previous quarter."
- **Contextualized chunk**: "This chunk is from an SEC filing on ACME corp's performance in Q2 2023; the previous quarter's revenue was $314 million. The company's revenue grew by 3% over the previous quarter."

The context is generated by an LLM that sees the ~8000 token segment and the specific chunk.

### 2. Contextual BM25
Apply the same contextualization before building the BM25 lexical index. This is critical because BM25 catches exact matches that semantic embeddings miss.

**Why BM25 matters**: Embeddings capture semantic meaning but miss exact matches. If a user queries "Error code TS-999", embeddings might return general error documentation while missing the exact TS-999 reference. BM25 finds the precise text match.

## Hybrid Retrieval with Rank Fusion

Combine both retrieval methods:
1. Get top chunks via semantic search (FAISS on contextualized embeddings)
2. Get top chunks via lexical search (BM25 on contextualized text)
3. Merge and deduplicate using Reciprocal Rank Fusion
4. Return top-k results

## Performance (from Anthropic's experiments)

- Contextual Embeddings alone: **35% reduction** in failed retrievals
- Contextual Embeddings + Contextual BM25: **49% reduction**
- Adding reranking: **67% reduction**

Tested across codebases, fiction, ArXiv papers, and science papers.

## Key Implementation Considerations

**Default document/chunk structure**:
- Segment size: ~8000 tokens
- Segment overlap: 10% (800 tokens)
- Chunk size: 800 tokens
- Chunk overlap: 10% (80 tokens)
- No word breakage at boundaries
- Context prepended: 100-200 tokens per chunk
- Context generated using the entire 8000 token segment as reference

For documents larger than 8000 tokens, split into overlapping 8000-token segments, then chunk each segment with overlap.

**Default models**:
- Embedding: text-embedding-3-small
- Context generation: gpt-4.1

**Number of retrieved chunks**: Default top-k = 20. Anthropic's tests showed 20 chunks performed better than 10 or 5.

**Custom context prompts**: The generic prompt works well, but domain-specific prompts can improve results (e.g., including glossary of terms for specialized fields).

**Embedding model choice**: Gemini and Voyage embeddings performed best in their tests. We use OpenAI for simplicity.

**Cost reduction via prompt caching**: Structure the prompt so the segment (8000 tokens) comes first, then the chunk. OpenAI automatically caches repeated prompt prefixes. When processing 10 chunks from same segment, the segment portion is cached after first call. Estimated ~$1 per million document tokens with caching.

## Modularity

All components are optional and can be enabled/disabled:

**At index creation time**:
- Context generation: can skip (falls back to standard RAG)
- FAISS index: can disable
- BM25 index: can disable

**At retrieval time**:
- Use FAISS only (semantic search)
- Use BM25 only (lexical search)
- Use hybrid (both + rank fusion)

This allows experimentation and comparison between standard RAG vs contextual RAG, semantic vs lexical vs hybrid.

## Target Users

Small team on local server. Not enterprise scale.

## Core Concepts

**Project**: Isolated knowledge base with its own documents, contextualized chunks, and indexes. Stored in its own directory.

**Segment**: An ~8000 token portion of a large document. Documents larger than 8000 tokens are split into overlapping segments.

**Chunk**: An 800 token portion of a segment. Each segment produces ~10 chunks.

**Contextual Chunk**: Original chunk text + LLM-generated situating context prepended together.

**Context Generation Prompt**: LLM receives the full segment (~8000 tokens) + specific chunk, outputs brief context (100-200 tokens) to situate chunk within segment. 

Original prompt from Anthropic's article:
```
<document>
{{WHOLE_DOCUMENT}}
</document>
Here is the chunk we want to situate within the whole document
<chunk>
{{CHUNK_CONTENT}}
</chunk>
Please give a short succinct context to situate this chunk within
the overall document for the purposes of improving search retrieval
of the chunk. Answer only with the succinct context and nothing else.
```

For tariff domain, we customize this prompt with domain-specific instructions.

## Requirements

### Project Management
- Create, list, load, delete projects by name

### Document Handling  
- Accept PDF, TXT, MD files
- Segment large documents into ~8000 token units
- Chunk each segment into 800 token chunks
- Track source metadata (file, segment, position)

### Context Generation
- Optional (can skip for standard RAG behavior)
- Generate 100-200 token context per chunk
- Use the entire 8000 token segment as reference for context generation
- Batch processing for cost efficiency
- Custom prompt support per project

### Indexing
- FAISS index on contextualized embeddings (optional)
- BM25 index on contextualized text (optional)
- At least one index required
- Persist and load from disk

### Retrieval
- Semantic-only mode (FAISS)
- Lexical-only mode (BM25)
- Hybrid mode (both + rank fusion)
- Fallback with warning if requested mode unavailable (e.g., hybrid requested but only FAISS enabled)
- Configurable top-k and fusion weights
- Result schema designed for agent decision-making (confidence, relevance indicators)

## Out of Scope
- REST API
- Reranking (can add later)
- Incremental index updates

## Success Criteria
- Measurably better retrieval than standard RAG on same documents
- Works for projects with hundreds of documents
- Easy integration with Agno agents
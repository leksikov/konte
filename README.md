# Konte

Contextual RAG library that improves retrieval by prepending LLM-generated context to chunks before embedding and indexing.

## Problem

Traditional RAG destroys context when chunking. A chunk like "The company's revenue grew by 3% over the previous quarter" lacks information about which company or quarter, causing retrieval failures.

## Solution

Konte implements [Anthropic's Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval) approach:

1. **Contextual Embeddings** - Before embedding each chunk, prepend a short context (100-200 tokens) generated by an LLM that sees the full segment (~8000 tokens)
2. **Contextual BM25** - Apply the same contextualization before building the BM25 lexical index
3. **Hybrid Retrieval** - Combine semantic (FAISS) and lexical (BM25) search with reciprocal rank fusion

This achieves up to **49% reduction** in failed retrievals (67% with reranking).

## Installation

```bash
pip install -e .
```

Set your OpenAI API key:
```bash
export OPENAI_API_KEY=sk-...
```

## CLI

Konte includes a command-line interface:

```bash
# Create a project
konte create my_project

# Add documents
konte add my_project doc1.pdf doc2.md

# Build indexes
konte build my_project

# Query (retrieval only)
konte query my_project "What is the HS code for memory chips?"

# Ask (full RAG with LLM answer)
konte ask my_project "What is the HS code for memory chips?"
konte ask my_project "What is the HS code for DDR5 RAM?" --show-sources

# List projects
konte list

# Project info
konte info my_project

# Delete project
konte delete my_project
```

## Quick Start

```python
import asyncio
from pathlib import Path
from konte import Project

async def main():
    # Create a project
    project = Project.create("my_knowledge_base")

    # Add documents
    project.add_documents([
        Path("doc1.pdf"),
        Path("doc2.txt"),
        Path("doc3.md"),
    ])

    # Build indexes (generates context, creates FAISS + BM25 indexes)
    await project.build()

    # Save project for later use
    project.save()

    # Query
    response = project.query("What was the revenue growth?")

    for result in response.results:
        print(f"[{result.score:.2f}] {result.content[:200]}...")

    # Check agent hints
    print(f"Suggested action: {response.suggested_action}")
    print(f"High confidence: {response.has_high_confidence}")

asyncio.run(main())
```

## Loading Existing Projects

```python
from konte import Project, get_project

# Option 1: Using Project.open()
project = Project.open("my_knowledge_base")

# Option 2: Using manager function
project = get_project("my_knowledge_base")

# Query immediately
response = project.query("your query here")
```

## Project Management

```python
from konte import (
    create_project,
    list_projects,
    get_project,
    delete_project,
    project_exists,
)

# Create new project
project = create_project("my_project")

# List all projects
projects = list_projects()  # Returns: ["my_project", ...]

# Check if project exists
if project_exists("my_project"):
    project = get_project("my_project")

# Delete project
delete_project("my_project")
```

## Retrieval Modes

```python
# Hybrid (default) - FAISS + BM25 with rank fusion
response = project.query("query", mode="hybrid")

# Semantic only - FAISS embeddings
response = project.query("query", mode="semantic")

# Lexical only - BM25 keyword matching
response = project.query("query", mode="lexical")
```

## Skip Context Generation

For standard RAG without LLM-generated context (faster, cheaper):

```python
await project.build(skip_context=False)
```

## Performance Optimizations

- **LLM Instance Caching**: Reuses ChatOpenAI instance to enable OpenAI prompt caching
- **Batch Processing**: Uses LangChain's `abatch()` for parallel LLM calls
- **Prompt Structure**: Segment (~8000 tokens) comes first enabling prefix caching across chunks

## Index Options

```python
# FAISS only (no BM25)
project = Project.create("semantic_only", enable_bm25=False)

# BM25 only (no FAISS) - no embeddings needed
project = Project.create("lexical_only", enable_faiss=False)
```

## Configuration

Set via environment variables or `.env` file:

```bash
OPENAI_API_KEY=sk-...
STORAGE_PATH=~/.konte          # Project storage location
EMBEDDING_MODEL=text-embedding-3-small
CONTEXT_MODEL=gpt-4.1
DEFAULT_TOP_K=20

# BackendAI (for context/answer generation)
BACKENDAI_ENDPOINT=https://qwen3vl.asia03.app.backend.ai/v1
BACKENDAI_MODEL_NAME=Qwen3-VL-8B-Instruct
```

## RAG Answer Generation

Generate LLM-grounded answers from retrieved chunks:

```python
import asyncio
from konte import Project

async def main():
    project = Project.open("my_project")

    # Full RAG pipeline: retrieval + LLM answer
    response, answer = await project.query_with_answer(
        query="What is the HS code for DDR5 RAM?",
        mode="hybrid",
        max_chunks=5,
    )

    print(answer.answer)       # LLM-generated answer
    print(answer.model)        # Model used (e.g., "Qwen3-VL-8B-Instruct")
    print(answer.sources_used) # Number of chunks used

asyncio.run(main())
```

By default uses BackendAI (Qwen3-VL-8B-Instruct), falls back to OpenAI if not configured.

## Agent Integration

Konte returns retrieval responses with decision hints for agent workflows:

```python
response = project.query("query")

# Suggested action based on confidence
print(response.suggested_action)
# "deliver" (score >= 0.7), "query_more" (0.4-0.7), or "refine_query" (< 0.4)

print(response.has_high_confidence)  # True if top_score >= 0.7
print(response.top_score)            # Highest result score (0-1)
print(response.score_spread)         # Difference between top and bottom scores

# Use as callable for Agno
retriever = project.as_retriever()
response = retriever("my query")
```

## RetrievalResponse Schema

```python
@dataclass
class RetrievalResponse:
    results: list[RetrievalResult]  # Ranked results
    query: str                       # Original query
    total_found: int                 # Number of results
    top_score: float                 # Highest score (0-1)
    score_spread: float              # Score range
    has_high_confidence: bool        # top_score >= 0.7
    suggested_action: str            # "deliver", "query_more", "refine_query"

@dataclass
class RetrievalResult:
    content: str      # Original chunk text
    context: str      # LLM-generated context
    score: float      # Relevance score (0-1)
    source: str       # Source filename
    chunk_id: str     # Unique chunk identifier
    metadata: dict    # Additional metadata
```

## Multi-Project Retrieval

Query multiple knowledge bases in parallel:

```python
from konte import Project

projects = [
    Project.open("hs_codes"),
    Project.open("gri_rules"),
    Project.open("precedents"),
]

# Query all projects
results = {}
for project in projects:
    results[project._config.name] = project.query("memory chip classification")

# Merge and rank
all_results = []
for name, response in results.items():
    for r in response.results:
        all_results.append((name, r))

all_results.sort(key=lambda x: x[1].score, reverse=True)
```

See [examples/parallel_multi_project_retrieval.py](examples/parallel_multi_project_retrieval.py) for a complete example.

## Agent Integration

Konte integrates with LangChain and Agno agent frameworks. See the [Agent Integration Guide](docs/AGENT_INTEGRATION_GUIDE.md) for:

- LangChain RAG chains and custom retrievers
- Agno tools and multi-project agents
- Confidence-based agent decisions
- Streaming responses

## Evaluation

RAG evaluation using DeepEval with LLM-as-judge metrics. **Current best: 96.7% accuracy** (29/30).

### Best Configuration

| Setting | Value |
|---------|-------|
| Test Cases | `synthetic_goldens_30.json` (30 validated) |
| Test Generation | HS code extraction + retrieval validation |
| Metric | DeepEval G-Eval FactualCorrectness |
| Reranking | Binary filter with fallback |
| Initial K | 100, Final K | 15 |

### Running Evaluation

```bash
# Generate 30 validated test cases
python -m evaluation.synthesize_korean_dataset \
  --project wco_hs_explanatory_notes_korean \
  --output evaluation/data/synthetic/synthetic_goldens_30.json \
  --num 30 \
  --seed 123

# Run LLM reranking + DeepEval
python -m evaluation.experiments.llm_reranking \
  --project wco_hs_explanatory_notes_korean \
  --test-cases evaluation/data/synthetic/synthetic_goldens_30.json \
  --method binary --initial-k 100 --final-k 15

python -m evaluation.experiments.run_deepeval_full binary 30_v2
```

### Results Summary

| Metric | Value |
|--------|-------|
| Pass Rate | **96.7%** |
| Passed | 29/30 |
| Avg Score | 0.933 |

See [evaluation/EVALUATION_REPORT.md](evaluation/EVALUATION_REPORT.md) for detailed methodology and failure analysis.

## Architecture

See the system flowcharts for detailed architecture:
- [High-level flowchart](high_level_system_flowchart.md)
- [Low-level flowchart](low_level_system_flowchart.md)

## License

MIT

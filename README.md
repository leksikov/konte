# Konte

Contextual RAG library that improves retrieval by prepending LLM-generated context to chunks before embedding and indexing.

## Problem

Traditional RAG destroys context when chunking. A chunk like "The company's revenue grew by 3% over the previous quarter" lacks information about which company or quarter, causing retrieval failures.

## Solution

Konte implements [Anthropic's Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval) approach:

1. **Contextual Embeddings** - Before embedding each chunk, prepend a short context (100-200 tokens) generated by an LLM that sees the full segment (~8000 tokens)
2. **Contextual BM25** - Apply the same contextualization before building the BM25 lexical index
3. **Hybrid Retrieval** - Combine semantic (FAISS) and lexical (BM25) search with reciprocal rank fusion

This achieves up to **49% reduction** in failed retrievals (67% with reranking).

## Architecture

See the system flowcharts for detailed architecture:
- [High-level flowchart](high_level_system_flowchart.md) - Overview of ingestion, context generation, indexing, and retrieval
- [Low-level flowchart](low_level_system_flowchart.md) - Detailed implementation flow including token counting, error handling, and score calculation

## Installation

```bash
pip install -e .
```

## Quick Start

```python
from konte import ProjectManager

# Create a project
manager = ProjectManager()
project = manager.create_project("my_knowledge_base")

# Add documents
await project.add_documents(["doc1.pdf", "doc2.txt", "doc3.md"])

# Build indexes (generates context, creates FAISS + BM25 indexes)
await project.build()

# Query
response = await project.query("What was the revenue growth?")

for result in response.results:
    print(f"[{result.score:.2f}] {result.content[:200]}...")
```

## Retrieval Modes

```python
# Hybrid (default) - FAISS + BM25 with rank fusion
response = await project.query("query", mode="hybrid")

# Semantic only - FAISS
response = await project.query("query", mode="semantic")

# Lexical only - BM25
response = await project.query("query", mode="lexical")
```

## Configuration

Set via environment variables or `.env` file:

```bash
OPENAI_API_KEY=sk-...
STORAGE_PATH=~/.konte          # Project storage location
EMBEDDING_MODEL=text-embedding-3-small
CONTEXT_MODEL=gpt-4.1
DEFAULT_TOP_K=20
```

## Agent Integration

Konte returns retrieval responses with decision hints for agent workflows:

```python
response = await project.query("query")

print(response.suggested_action)  # "deliver", "query_more", or "refine_query"
print(response.has_high_confidence)  # True if top_score >= 0.7
print(response.top_score)  # Highest result score
print(response.score_spread)  # Difference between top and bottom scores
```

## License

MIT

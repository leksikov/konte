# Konte

Contextual RAG library that improves retrieval by prepending LLM-generated context to chunks before embedding and indexing.

## Problem

Traditional RAG destroys context when chunking. A chunk like "The company's revenue grew by 3% over the previous quarter" lacks information about which company or quarter, causing retrieval failures.

## Solution

Konte implements [Anthropic's Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval) approach:

1. **Contextual Embeddings** - Before embedding each chunk, prepend a short context (100-200 tokens) generated by an LLM that sees the full segment (~8000 tokens)
2. **Contextual BM25** - Apply the same contextualization before building the BM25 lexical index
3. **Hybrid Retrieval** - Combine semantic (FAISS) and lexical (BM25) search with reciprocal rank fusion

This achieves up to **49% reduction** in failed retrievals (67% with reranking).

## Installation

```bash
pip install -e .
```

Optionally set your OpenAI API key (Backend.AI is used by default):
```bash
export OPENAI_API_KEY=sk-...  # Optional, for OpenAI embeddings
```

## CLI

Konte includes a command-line interface:

```bash
# Create a project
konte create my_project

# Add documents
konte add my_project doc1.pdf doc2.md

# Build indexes
konte build my_project

# Query (retrieval only)
konte query my_project "What is the HS code for memory chips?"

# Ask (full RAG with LLM answer)
konte ask my_project "What is the HS code for memory chips?"
konte ask my_project "What is the HS code for DDR5 RAM?" --show-sources

# List projects
konte list

# Project info
konte info my_project

# Delete project
konte delete my_project
```

## Quick Start

```python
import asyncio
from pathlib import Path
from konte import Project

async def main():
    # Create a project
    project = Project.create("my_knowledge_base")

    # Add documents
    project.add_documents([
        Path("doc1.pdf"),
        Path("doc2.txt"),
        Path("doc3.md"),
    ])

    # Build indexes (generates context, creates FAISS + BM25 indexes)
    await project.build()

    # Save project for later use
    project.save()

    # Query
    response = project.query("What was the revenue growth?")

    for result in response.results:
        print(f"[{result.score:.2f}] {result.content[:200]}...")

    # Check agent hints
    print(f"Suggested action: {response.suggested_action}")
    print(f"High confidence: {response.has_high_confidence}")

asyncio.run(main())
```

## Loading Existing Projects

```python
from konte import Project, get_project

# Option 1: Using Project.open()
project = Project.open("my_knowledge_base")

# Option 2: Using manager function
project = get_project("my_knowledge_base")

# Query immediately
response = project.query("your query here")
```

## Project Management

```python
from konte import (
    create_project,
    list_projects,
    get_project,
    delete_project,
    project_exists,
)

# Create new project
project = create_project("my_project")

# List all projects
projects = list_projects()  # Returns: ["my_project", ...]

# Check if project exists
if project_exists("my_project"):
    project = get_project("my_project")

# Delete project
delete_project("my_project")
```

## Retrieval Modes

```python
# Hybrid (default) - FAISS + BM25 with rank fusion
response = project.query("query", mode="hybrid")

# Semantic only - FAISS embeddings
response = project.query("query", mode="semantic")

# Lexical only - BM25 keyword matching
response = project.query("query", mode="lexical")
```

## Skip Context Generation

For standard RAG without LLM-generated context (faster, cheaper):

```python
await project.build(skip_context=True)
```

## Performance Optimizations

### vLLM/OpenAI Prefix Caching

Context generation is optimized for KV cache prefix caching:

```
Prompt structure: [SEGMENT ~8000 tokens] + [CHUNK ~800 tokens]
```

**How it works:**
1. All chunks within a segment share the same prefix (segment text)
2. Chunks are sent in parallel via `abatch(max_concurrency=len(chunks))`
3. First request computes and caches the segment prefix KV states
4. Subsequent chunk requests hit the cache - only compute the unique chunk suffix
5. Segments are processed sequentially to maximize cache efficiency

```
Segment A (10 chunks):
  Request 1: segment_A + chunk_1  → compute prefix, cache it
  Request 2: segment_A + chunk_2  → cache hit, compute only chunk_2
  Request 3: segment_A + chunk_3  → cache hit, compute only chunk_3
  ...
Then Segment B, etc.
```

Request order within a segment doesn't matter - whichever arrives first triggers caching.

### Other Optimizations

- **LLM Instance Caching**: Reuses ChatOpenAI instance across calls
- **Batch Processing**: Uses LangChain's `abatch()` for parallel LLM calls within segment

## Logging

Structured logging via structlog provides visibility into the ingestion pipeline:

```
2024-01-15 10:30:01 [info] loading_document path=/data/doc.pdf
2024-01-15 10:30:02 [info] document_chunked path=/data/doc.pdf num_chunks=55
2024-01-15 10:30:02 [info] context_generation_started total_segments=5 skip_context=False
2024-01-15 10:30:03 [info] generating_context_for_segment segment_key=('doc.pdf', 0) total_segments=5 num_chunks=11
...
2024-01-15 10:30:15 [info] context_generation_complete num_chunks=55 skipped=False
2024-01-15 10:30:16 [info] faiss_index_built
2024-01-15 10:30:16 [info] project_build_complete
```

Pipeline: 1 document → 5 segments (~8000 tokens each) → ~11 chunks per segment (~800 tokens) = 55 chunks total. Use debug level for granular token counts.

## Index Options

```python
# FAISS only (no BM25)
project = Project.create("semantic_only", enable_bm25=False)

# BM25 only (no FAISS) - no embeddings needed
project = Project.create("lexical_only", enable_faiss=False)
```

## Configuration

Set via environment variables or `.env` file:

```bash
OPENAI_API_KEY=sk-...
STORAGE_PATH=~/.konte          # Project storage location
EMBEDDING_MODEL=text-embedding-3-small
CONTEXT_MODEL=gpt-4.1
DEFAULT_TOP_K=20

# BackendAI (for context/answer generation)
BACKENDAI_ENDPOINT=https://qwen3vl.asia03.app.backend.ai/v1
BACKENDAI_MODEL_NAME=Qwen3-VL-8B-Instruct
```

## RAG Answer Generation

Generate LLM-grounded answers from retrieved chunks:

```python
import asyncio
from konte import Project

async def main():
    project = Project.open("my_project")

    # Full RAG pipeline: retrieval + LLM answer
    response, answer = await project.query_with_answer(
        query="What is the HS code for DDR5 RAM?",
        mode="hybrid",
        max_chunks=5,
    )

    print(answer.answer)       # LLM-generated answer
    print(answer.model)        # Model used (e.g., "Qwen3-VL-8B-Instruct")
    print(answer.sources_used) # Number of chunks used

asyncio.run(main())
```

By default uses BackendAI (Qwen3-VL-8B-Instruct), falls back to OpenAI if not configured.

## Agent Integration

Konte returns retrieval responses with decision hints for agent workflows:

```python
response = project.query("query")

# Suggested action based on confidence
print(response.suggested_action)
# "deliver" (score >= 0.7), "query_more" (0.4-0.7), or "refine_query" (< 0.4)

print(response.has_high_confidence)  # True if top_score >= 0.7
print(response.top_score)            # Highest result score (0-1)
print(response.score_spread)         # Difference between top and bottom scores

# Use as callable for Agno
retriever = project.as_retriever()
response = retriever("my query")
```

## RetrievalResponse Schema

```python
@dataclass
class RetrievalResponse:
    results: list[RetrievalResult]  # Ranked results
    query: str                       # Original query
    total_found: int                 # Number of results
    top_score: float                 # Highest score (0-1)
    score_spread: float              # Score range
    has_high_confidence: bool        # top_score >= 0.7
    suggested_action: str            # "deliver", "query_more", "refine_query"

@dataclass
class RetrievalResult:
    content: str      # Original chunk text
    context: str      # LLM-generated context
    score: float      # Relevance score (0-1)
    source: str       # Source filename
    chunk_id: str     # Unique chunk identifier
    metadata: dict    # Additional metadata
```

## Multi-Project Retrieval

Query multiple knowledge bases in parallel:

```python
from konte import Project

projects = [
    Project.open("hs_codes"),
    Project.open("gri_rules"),
    Project.open("precedents"),
]

# Query all projects
results = {}
for project in projects:
    results[project.config.name] = project.query("memory chip classification")

# Merge and rank
all_results = []
for name, response in results.items():
    for r in response.results:
        all_results.append((name, r))

all_results.sort(key=lambda x: x[1].score, reverse=True)
```

See [examples/parallel_multi_project_retrieval.py](examples/parallel_multi_project_retrieval.py) for a complete example.

## Examples

| Example | Demonstrates |
|---------|-------------|
| [basic_usage.py](examples/basic_usage.py) | Project CRUD, document loading, building, querying, retrieval modes |
| [query_with_answer.py](examples/query_with_answer.py) | Full RAG pipeline, custom prompt templates, GeneratedAnswer model |
| [metadata_filtering.py](examples/metadata_filtering.py) | source_filter, metadata_filter, combining filters with modes |
| [async_reranking.py](examples/async_reranking.py) | Async querying, LLM reranking, comparing with/without reranking |
| [parallel_multi_project_retrieval.py](examples/parallel_multi_project_retrieval.py) | Multi-project querying, result merging |

## Agent Integration

Konte integrates with LangChain and Agno agent frameworks. See the [Agent Integration Guide](docs/AGENT_INTEGRATION_GUIDE.md) for:

- LangChain RAG chains and custom retrievers
- Agno tools and multi-project agents
- Confidence-based agent decisions
- Streaming responses

## Evaluation

RAG evaluation using DeepEval with LLM-as-judge metrics. **Current best: 97-99% accuracy**.

### Two Evaluation Types

| Evaluation | Dataset | Questions | Pass Rate | Avg Score |
|------------|---------|-----------|-----------|-----------|
| **Diverse RAG** | `deepeval_goldens_korean_no_hypothetical.json` | 70 | **98.6%** | **0.831** |
| **HS Code Lookup** | `synthetic_goldens_100_fixed.json` | 100 | **97.0%** | **0.940** |

**Notes**: Diverse dataset filtered (30 hypothetical questions removed - require inference beyond source documents). HS Code dataset has 3 ground truth errors fixed.

### Quick Start

```bash
# Run evaluation on diverse questions (recommended)
python -m evaluation.experiments.llm_reranking \
  --project wco_hs_explanatory_notes_korean \
  --test-cases evaluation/data/synthetic/deepeval_goldens_korean_no_hypothetical.json \
  --method binary --initial-k 100 --final-k 15 --max-cases 0 \
  --output evaluation/experiments/results/llm_rerank_binary_deepeval_diverse.json

python -m evaluation.experiments.run_deepeval_full binary deepeval_diverse answer

# Or for HS code evaluation
python -m evaluation.experiments.run_deepeval_full binary 100 hs_code
```

See [evaluation/EVALUATION_GUIDE.md](evaluation/EVALUATION_GUIDE.md) for test generation and detailed methodology.

## Architecture

See the system flowcharts for detailed architecture:
- [High-level flowchart](high_level_system_flowchart.md)
- [Low-level flowchart](low_level_system_flowchart.md)

## License

MIT
